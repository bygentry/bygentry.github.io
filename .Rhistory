# report beta coefficients
print(m1$coefficients)
#| label: Packages
#| message: false
#| code-fold: true
library(curl)
library(manipulate)
library(ggplot2)
library(broom)
library(lmtest)
library(tidyverse)
#| label: Data
#| code-fold: show
d <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/refs/heads/master/AN588_Spring25/KamilarAndCooperData.csv")
df <- read.csv(d, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(df)
#| label: Linear Regression
# creating new reference vars
range <- df$HomeRange_km2
f_mass <- df$Body_mass_female_mean
# create a linear model, m1, with relation y ~ x for the variables log(HomeRange_km2) and log(Body_mass_female_mean)
m1 <- lm(log(range) ~ log(f_mass), data = df)
# report beta coefficients
print(m1$coefficients)
# report beta coefficients
table(m1$coefficients)
list(SE = boot_se, CI = boot_ci)
#| label: Bootstrapping
# Set number of bootstraps, n
n <- 1000
# Store coefficients from each bootstrap
boot_coefs <- replicate(n, {
s <- df %>% sample_frac(replace = TRUE)
coef(lm(log(HomeRange_km2) ~ log(Body_mass_female_mean), data = s))
}, simplify = "matrix")
# Convert to data frame
boot_df <- as.data.frame(t(boot_coefs))
colnames(boot_df) <- c("Intercept", "Slope")
# Estimate SEs and CIs
boot_se <- apply(boot_df, 2, sd)
boot_ci <- apply(boot_df, 2, quantile, probs = c(0.025, 0.975))
list(SE = boot_se, CI = boot_ci)
#| label: Comparing SEs and CIs
#| code-fold: true
list(LM.SE =summary(m1)$coefficients[, "Std. Error"], Boot.SE = boot_se); list(LM.CI = confint(m1), Boot.CI = boot_ci)
#| label: Packages
#| message: false
# data handling
library(curl)
library(tidyverse)
# discriminant analysis
library(MASS)
# data visualization
library(ggplot2)
library(plotly)
library(geometry)
library(mulgar)
library(ggpubr)
#| label: Simulating Data
# First, let's set a seed for reproducibility,
set.seed(647)
# then set n as the number of observations to simulate.
n <- 300
# Now we can simulate the transcription frequencies.
# Let's assume that the classification is set up as such:
# Genotype A: higher expression of gene X
# Genotype B: higher expression of gene Y
# Genotype C: higher expression of gene Z
geno <- sample(c(0, 1, 2), size = n, replace = TRUE)
# now the gene expressions
gene_x <- rnorm(n, mean = 5 + (2 * (geno = 0)), sd = 1)
# produces a distribution that is higher in genotype A individuals
gene_y <- rnorm(n, mean = 5 + (2 * (geno = 1)), sd = 1)
# produces a distribution that is higher in genotype A individuals
gene_z <- rnorm(n, mean = 5 + (2 * (geno = 2)), sd = 1)
# produces a distribution that is higher in genotype C individuals
# Now, let's store this simulated data
lda_data <- data.frame(
gene_x = gene_x,
gene_y = gene_y,
gene_z = gene_z,
geno = factor(geno, labels = c("B", "A", "C"))
)
#| label: Simulating Data
# First, let's set a seed for reproducibility,
set.seed(647)
# then set n as the number of observations to simulate.
n <- 300
# Now we can simulate the transcription frequencies.
# Let's assume that the classification is set up as such:
# Genotype A: higher expression of gene X
# Genotype B: higher expression of gene Y
# Genotype C: higher expression of gene Z
geno <- sample(c(0, 1, 2), size = n, replace = TRUE)
# now the gene expressions
gene_x <- rnorm(n, mean = 5 + (2 * (geno == 0)), sd = 1)
# produces a distribution that is higher in genotype A individuals
gene_y <- rnorm(n, mean = 5 + (2 * (geno == 1)), sd = 1)
# produces a distribution that is higher in genotype A individuals
gene_z <- rnorm(n, mean = 5 + (2 * (geno == 2)), sd = 1)
# produces a distribution that is higher in genotype C individuals
# Now, let's store this simulated data
lda_data <- data.frame(
gene_x = gene_x,
gene_y = gene_y,
gene_z = gene_z,
geno = factor(geno, labels = c("A", "B", "C"))
)
# and lastly, check the first few rows to make sure everything is right
head(lda_data)
#| label: Calculating Means
mu1 <- colMeans(lda_data[lda_data$geno == "A", 1:2])
mu2 <- colMeans(lda_data[lda_data$geno == "B", 1:2])
#| label: Pooling Covariance
S1 <- cov(lda_data[lda_data$geno == "A", 1:2])
# covariance of class A in gene x and gene y
S2 <- cov(lda_data[lda_data$geno == "B", 1:2])
# covariance of class B in gene x and gene y
n1 <- sum(lda_data$geno == "A")
# size of class A
n2 <- sum(lda_data$geno == "B")
# size of class B
cov_pooled <- ((n1 - 1)*S1 + (n2 - 1)*S2) / (n1 + n2 - 2)
cov_pooled
#| label: Inverting Covariance Matrix
cov_inv <- solve(cov_pooled)
#| label: Calculating Eigenvalues
d <- (mu1 - mu2)
w <- as.vector(cov_inv %*% d)
b <- -0.5 * t(mu1 + mu2) %*% cov_inv %*% d
#| label: Visualizing Linear Boundary
#| warning: false
#| code-fold: true
# Filter out geno C data
lda_data_xy <- lda_data[lda_data$geno != "C", ]
# Create a grid to visualize the boundary
x_vals <- seq(min(lda_data_xy$gene_x), max(lda_data_xy$gene_x),
length.out = 100)
y_vals <- (-w[1] * x_vals - b) / w[2]  # Solving for y
boundary <- data.frame(gene_x = x_vals, gene_y = as.vector(y_vals))
ggplot(lda_data_xy, aes(x = gene_x, y = gene_y, color = geno)) +
geom_point()  +
geom_line(data = boundary, aes(x = x_vals, y = y_vals),
color = "black", linetype = "dashed") +
labs(title = "LDA Boundary for gene_x vs gene_y") +
coord_fixed() +
theme_bw()
#| label: Normalizing Eigenvector
w_unit <- w/sqrt(sum(w^2))
#| label: Centering Classes
X <- as.matrix(lda_data_xy[, c("gene_x", "gene_y")])
# Center around the global mean
X_centered <- scale(X, center = TRUE, scale = FALSE)
#| label: Creating LDA Axis
# Projection of each point onto the LDA axis
lda_proj <- X_centered %*% w_unit
lda_data_xy$lda_proj <- as.vector(lda_proj)
#| label: Visualizing the Discriminant Axis
#| code-fold: true
proj_coords <- X_centered %*% w_unit %*% t(w_unit)
global_mean <- colMeans(X)
proj_coords <- sweep(proj_coords, 2, global_mean, FUN = "+")
# Build a dataframe for plotting
proj_df <- data.frame(
gene_x_proj = proj_coords[, 1],
gene_y_proj = proj_coords[, 2],
geno = lda_data_xy$geno
)
# Add the projected points to the plot
ggplot() +
geom_point(data = lda_data_xy,
aes(x = gene_x, y = gene_y, color = geno),
alpha = 0.6, size = 2) +
geom_point(data = proj_df,
aes(x = gene_x_proj,
y = gene_y_proj,
color = geno),
shape = 4,
size = 2.5) +
geom_segment(aes(x = lda_data_xy$gene_x,
y = lda_data_xy$gene_y,
xend = proj_df$gene_x_proj,
yend = proj_df$gene_y_proj,
color = lda_data_xy$geno),
alpha = 0.4) +
labs(title = "Projection of Points onto LDA Axis",
x = "gene_x",
y = "gene_y") +
coord_fixed() +
theme_bw()
#| label: Visualizing Boundary and Discriminant Together
#| code-fold: true
ggplot() +
geom_point(data = lda_data_xy,
aes(x = gene_x,
y = gene_y,
color = geno),
alpha = 0.6, size = 2) +
geom_point(data = proj_df,
aes(x = gene_x_proj,
y = gene_y_proj,
color = geno),
shape = 4,
size = 2.5) +
geom_segment(aes(x = lda_data_xy$gene_x,
y = lda_data_xy$gene_y,
xend = proj_df$gene_x_proj,
yend = proj_df$gene_y_proj,
color = lda_data_xy$geno),
alpha = 0.4) +
geom_line(data = boundary,
aes(x = x_vals,
y = y_vals),
color = "black",
linetype = "dashed") +
labs(title = "LDA Boundary and Axis",
x = "gene_x",
y = "gene_y") +
coord_fixed() +
theme_bw()
#| label: Creating LDA Model
lda_model <- lda(geno ~ gene_x + gene_y + gene_z, data = lda_data)
# Check model output
print(lda_model)
#| label: Making Predictions
lda_pred <- predict(lda_model, lda_data)
# The LDA projection scores for each observation
lda_data$LD1 <- lda_pred$x[,1]
lda_data$LD2 <- lda_pred$x[,2]
#| label: Visualizing Multi-Class LDA
#| code-fold: true
# Create a grid of points to evaluate the decision boundary
grid <- expand.grid(LD1 = seq(min(lda_data$LD1) - 1,
max(lda_data$LD1) + 1,
length.out = 100),
LD2 = seq(min(lda_data$LD2) - 1,
max(lda_data$LD2) + 1,
length.out = 100))
grid$gene_x <- (grid$LD1)
# Placeholder for gene_x, gene_y, gene_z transformation
grid$gene_y <- (grid$LD2)
# Same for gene_y
grid$gene_z <- 1
# If you want to include gene_z as a constant, modify as necessary.
# Predict the class for each point in the grid
grid$pred_geno <- predict(lda_model, newdata = grid)$class
# Plot with decision boundary
ggplot(lda_data,
aes(x = LD1,
y = LD2,
color = geno)) +
geom_point(size = 3,
alpha = 0.7) +
labs(title = "2D LDA Projection with Decision Boundary",
x = "Linear Discriminant 1 (LD1)",
y = "Linear Discriminant 2 (LD2)") +
geom_contour(data = grid,
aes(x = LD1,
y = LD2,
z = as.numeric(pred_geno)),
color = "black",
bins = 1) +
theme_bw() +
theme(aspect.ratio = 1,
legend.title = element_blank())
#| label: Generating Class Ellipses
ell <- NULL
for(i in unique(lda_data$geno)){
x <- lda_data |> dplyr::filter(geno == i)
e <- gen_xvar_ellipse(x[, 1:3], n = 300, nstd = 1)
e$geno <- i
ell <- bind_rows(ell, e)
}
#| label: Plotting Ellipses
#| code-fold: true
lda1 <- ggplot(lda_data,
aes(x = gene_x,
y = gene_y,
color = geno)) +
geom_point() +
scale_color_discrete() +
ggtitle("(a)") +
theme_bw() +
theme(aspect.ratio = 1,
legend.title = element_blank())
lda2 <- ggplot(ell,
aes(x = gene_x,
y = gene_y,
color = geno)) +
geom_point() +
scale_color_discrete() +
ggtitle("(b)") +
theme_bw() +
theme(aspect.ratio = 1,
legend.title = element_blank())
ggarrange(lda1, lda2, ncol = 2,
common.legend = TRUE, legend = "bottom")
#| label: 3D Modeling of Multi-Class LDA
#| code-fold: true
# Create a 3D scatter plot with LD1, LD2, and LD3
fig <- plot_ly(data = ell,
x = ~gene_x,
y = ~gene_y,
z = ~gene_z,
color = ~geno,
type = 'scatter3d',
mode = 'markers',
marker = list(size = 5)) %>%
layout(title = "3D LDA Projection (LD1, LD2, LD3)",
scene = list(xaxis = list(title = 'LD1'),
yaxis = list(title = 'LD2')))
fig
plotly::json(fig)
plotly::plotly_json(fig)
install.packages("listviewer");library(listviewer)
library(listviewer)
plotly::plotly_json(fig)
#| label: 3D Modeling of Multi-Class LDA
#| code-fold: true
# Create a 3D scatter plot with LD1, LD2, and LD3
fig <- plot_ly(data = ell,
x = ~gene_x,
y = ~gene_y,
z = ~gene_z,
color = ~geno,
type = 'scatter3d',
mode = 'markers',
marker = list(size = 5)) %>%
layout(title = "3D LDA Projection",
scene = list(xaxis = list(title = 'Gene X'),
yaxis = list(title = 'Gene Y'),
zaxis = list(title = 'Gene Z')))
fig
plotly::plotly_json(fig)
fig
plotly::plotly_json(fig)
plotly::plotly_json(fig)
