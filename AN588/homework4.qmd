---
title: "What's Your Malfunction?"
author: "Gentry Miller"
format: 
  html:
    code-overflow: wrap
    theme: minty
    toc: true
    toc-depth: 2
    toc-location: left
    toc-title: Contents
    df-print: kable
    style: css
editor: visual
---

## Preliminaries

```{r}
#| label: Packages and Data
#| message: false

library(curl)
library(manipulate)
library(tidyverse)
library(ggplot2)
library(ggpubr)

d <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/refs/heads/master/AN588_Spring25/KamilarAndCooperData.csv")
df <- read.csv(d, header = TRUE, sep = ",", stringsAsFactors = FALSE)
```

## Question 1

```{r}
#| label: Z tests

# function Z.prop.test() that takes as parameters:

# p1 and n1, representing the est. proportion and sample size
# p2 and n2 (default = NULL) representing a second sample's proportion and sample size
# p0 as expected value for the pop. proportion
# alternative (default = "two.sided") and conf.level (default = 0.95)

# and returns a list of the test statistic (Z), the p value (P), and the confidence interval (CI) wrt conf.level

Z.prop.test <- function(p1, n1, p2 = NULL, n2 = NULL, p0, alternative = "two.sided", conf.level = 0.95){
  
  # normality check
  is.norm <- function(p, n){
    if (n * p < 5 || n * (1 - p) < 5) {
      warning("Distribution may not be normal: np and n(1-p) should both be > 5")
    }
  }
  
  # one sample
  if(is.null(p2) || is.null(n2)){
    # if p2 or n2 is null there is only one sample to test
    is.norm(p1, n1)
    
    p_hat <- p1
    SE <- sqrt(p0 * (1-p0) / n1)
    Z <- (p_hat - p0) / SE
    CI <- p_hat + c(-1,1) * qnorm(1 - (1 - conf.level) / 2) * SE
  } 
  
  # two samples
  else {
    # if neither p2 nor n2 is null there are two samples to test
    is.norm(p1, n1)
    is.norm(p2, n2)
    
    p_hat1 <- p1
    p_hat2 <- p2
    p_hats <- (p_hat1 * n1 + p_hat2 * n2) / (n1 + n2)
    SE <- sqrt(p_hats * (1 - p_hats) * (1 / n1 + 1 / n2))
    Z <- (p_hat1 - p_hat2) / SE
    CI <- (p_hat1 - p_hat2) + c(-1,1) * qnorm(1 - (1 - conf.level) / 2) * SE
  }
  
  # calculating p-value
  # identifies test type and calculates P value using appropriate formula
  # if invalid test type is passed to "alternative", stop and issue error
  if (alternative == "two.sided"){
    P <- 2 * (1 - pnorm(abs(Z)))
  } else if (alternative == "greater"){
    P <-  1 - pnorm(Z)
  } else if (alternative == "less"){
    P <- pnorm(Z)
  } else{
    stop("ERROR: Invalid test type. Alternative must be set to 'two.sided', 'greater', or 'less'")
  }
  
  return(list(Z = Z, P = P, CI = CI))
}
```

## Question 2

For this exercise, the end aim is to fit a simple linear regression model to predict longevity (`MaxLongevity_m`) measured in months from species’ brain size (`Brain_Size_Species_Mean`) measured in grams. Do the following for both `longevity~brain size` and `log(longevity)~log(brain size)` :

### 2.1

-   Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function `geom_text()`).

```{r}
#| label: Regression Model

# store data for each field in a holder var for easier access
l <- df$MaxLongevity_m
b <- df$Brain_Size_Species_Mean
log.l <- log(l)
log.b <- log(b)

# create the models
m <- lm(data = df, l ~ b)
log.m <- lm(data = df, log.l ~ log.b)

# get relevant values and generate equations
int <- coef(m)[1]
slope <- coef(m)[2]
log.int <- coef(log.m)[1]
log.slope <- coef(log.m)[2]

eq <- sprintf("l = %.2f + %.2f * b",
              int, slope)
log.eq <- sprintf("log(l) = %.2f + %.2f * log(b)",
                  log.int, log.slope)

```

```{r}
#| label: Plotting Regressions
#| warning: false
#| code-fold: true

# plot both models
temp1 <- ggplot(df, aes(x = b, y = l)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Mean Brain Size",
       y = "Max Longevity") +
  geom_text(aes(x = Inf, y = Inf, label = eq),
            hjust = 1.4, vjust = 2, 
            inherit.aes = FALSE,
            size = 3, color = "red3") +
  ggtitle("(a)") +
  theme_bw()

#hjust = 1.1, vjust = 2, 
temp2 <- ggplot(df, aes(x = log.b, y = log.l)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Log Mean Brain Size",
       y = "Log Max Longevity") +
  geom_text(aes(x = Inf, y = Inf, label = log.eq),
            hjust = 1.2, vjust = 2,
            inherit.aes = FALSE,
            size = 3, color = "red3") +
  ggtitle("(b)") +
  theme_bw()

comp <- ggarrange(temp1, temp2, ncol = 2)
comp
```

### 2.2

-   Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.

```{r}
#| label: Beta and t Values
#| output: false
#| code-fold: true

temp1 <- c(as.numeric(slope),
            as.numeric(summary(m)$coefficients[2,4]))
temp2 <- c(as.numeric(log.slope),
                as.numeric(summary(log.m)$coefficients[2,4]))

betas <- data.frame(normal = temp1,
                    log = temp2)

rownames(betas) <- c("β1", "Pr(>|t|)"); betas
```

```         
               normal          log 
β1             1.217990e+00    2.341496e-01 
Pr(>|t|)       2.685909e-20    2.165214e-25
```

> The point estimates of the slope ($\beta 1$) are the values that reflect the rate of change of the response variable for each unit increase of the predictor variable, so for every 1 unit increase in brain size the l \~ b model shows an increase in l by 1.22 and for every 1 unit increase in log(brain size) the log model shows a 0.23 increase in log(l). Since $\beta 1 \ne 0$ and the p values are well below .05, we reject the null hypothesis and can say that there is a significant relationship.

```{r}
#| label: Confidence Intervals
#| code-fold: true
#| output: false

cat(sprintf("m:     5%% %.3f   95%% %.3f\n", 
            confint(m, level = 0.9)[2, 1], 
            confint(m, level = 0.9)[2, 2]))

cat(sprintf("log.m: 5%% %.3f   95%% %.3f\n", 
            confint(log.m, level = 0.9)[2, 1], 
            confint(log.m, level = 0.9)[2, 2]))
```

```         
Confidence Intervals:
m:     5% 1.036   95% 1.400 
log.m: 5% 0.205   95% 0.264
```

### 2.3

-   Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.

```{r}
#| label: Confidence and Prediction Lines I - Calculations

# generate values for the CI lines
CI <- predict(m, newdata = data.frame(b), 
              interval = "confidence", 
              level = 0.9, na.omit = TRUE
              )

log.CI <- predict(log.m, newdata = data.frame(log.b),
                  interval = "confidence", 
                  level = 0.9, na.omit = TRUE
                  )

# generate values for the PI lines
PI <- predict(m, newdata = data.frame(b),
                  interval = "prediction", 
                  level = 0.9, na.omit = TRUE
              )

log.PI <- predict(log.m, newdata = data.frame(log.b),
                  interval = "prediction", 
                  level = 0.9, na.omit = TRUE
                  )

# store in data frame for plotting
ints <- data.frame(cbind(b, l, CI, PI)) %>% 
  rename(
    CI.Fit = fit,
    CI.Lower = lwr,
    CI.Upper = upr,
    PI.Fit = fit.1,
    PI.Lower = lwr.1,
    PI.Upper = upr.1
  )

log.ints <- data.frame(cbind(log.b, log.l, 
                             log.CI, log.PI)) %>% 
  rename(
    CI.Fit = fit,
    CI.Lower = lwr,
    CI.Upper = upr,
    PI.Fit = fit.1,
    PI.Lower = lwr.1,
    PI.Upper = upr.1)
```

```{r}
#| label: Confidence and Prediction Lines II - Data Prep
#| code-fold: true

# This is where I made some bts transformations to my
# data frame during the plot debugging process 

# pivots the interval columns to inform geom_scatter() 
# and geom_line() without having to do each line individually
ints.pivot <- ints %>%
  pivot_longer(cols = c("CI.Fit", "CI.Lower", "CI.Upper",
                        "PI.Fit", "PI.Lower", "PI.Upper"),
               names_to = "type",
               values_to = "vals") 

log.ints.pivot <- log.ints %>% 
  pivot_longer(cols = c("CI.Fit", "CI.Lower", "CI.Upper",
                        "PI.Fit", "PI.Lower", "PI.Upper"),
               names_to = "type",
               values_to = "vals") 

# creates a new value interval_group to simplify legend
ints.pivot <- ints.pivot %>% 
  mutate(interval_group = case_when(
    str_detect(type, "Fit") ~ "Fit",
    str_detect(type, "CI") ~ "CI",
    str_detect(type, "PI") ~ "PI"
  ))

log.ints.pivot <- log.ints.pivot %>% 
  mutate(interval_group = case_when(
    str_detect(type, "Fit") ~ "Fit",
    str_detect(type, "CI") ~ "CI",
    str_detect(type, "PI") ~ "PI"
  ))
```

```{r}
#| label: Confidence and Prediction Lines III - Plotting
#| code-fold: true
#| warning: false

temp1 <- ggplot(data = ints.pivot, aes(x = b)) +
  geom_point(data = ints, aes(x = b, y = l)) +
  geom_line(aes(y = vals, 
                color = interval_group,
                group = type)) +
  labs(x = "Mean Brain Size",
       y = "Max Longevity",
       color = "Interval Type") +
  scale_color_manual(values = c("Fit" = "black",
                                "CI" = "blue3",
                                "PI" = "red3")) +
  ggtitle("(a)") +
  theme_bw() +
  theme(legend.title = element_blank())

temp2 <- ggplot(data = log.ints.pivot, aes(x = log.b)) +
  geom_point(data = log.ints, aes(x = log.b, y = log.l)) +
  geom_line(aes(y = vals, 
                color = interval_group, 
                group = type)) +
  labs(x = "Log Mean Brain Size",
       y = "Log Max Longevity",
       color = "Interval Type") +
  scale_color_manual(values = c("Fit" = "black",
                                "CI" = "blue3",
                                "PI" = "red3")) +
  ggtitle("(b)") +
  theme_bw() + 
  theme(legend.title = element_blank())

ggarrange(temp1, temp2, ncol = 2,
          common.legend = TRUE, legend = "bottom")
```

### 2.4

-   Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

```{r}
#| label: Making Predictions
#| warning: false

temp1 <- data.frame(Brain_Size_Species_Mean = 800)

est <- data.frame(predict(m, newdata = temp1,
                          interval = "prediction",
                          level = .9))

log.est <- data.frame(predict(log.m, newdata = temp1,
                              interval = "prediction",
                              level = .9))
temp1 <- c(est[1, "fit"], 
           est[1, "lwr"], 
           est[1, "upr"])

temp2 <- c(exp(log.est[1, "fit"]), 
           exp(log.est[1, "lwr"]), 
           exp(log.est[1, "upr"]))

ests <- data.frame(normal = temp1,
           log = temp2)
rownames(ests) <- c("Estimate", "Lower", "Upper")
ests
```

### 2.4.2

-   Looking at your two models, which do you think is better? Why?

```{r}
#| label: Comparing Models
#| code-fold: true

comp

summary(m)
summary(log.m)
```

> Based on the summaries and visualization, I think the log model is better. The log model has higher R^2^ and adjusted R^2^ values and lower p values compared to the non-logged model suggesting a better fit to this data.
